{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Deep Learning](https://www.cc.gatech.edu/~hays/compvision/proj6/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "import student_code as sc\n",
    "from torchvision.models import alexnet\n",
    "\n",
    "data_path = osp.join('../data', '15SceneData')\n",
    "num_classes = 15\n",
    "\n",
    "# If you have a good Nvidia GPU with an appropriate environment, \n",
    "# try setting the use_GPU flag to True (the environment provided does\n",
    "# not support GPUs and we will not provide any support for GPU\n",
    "# computation in this project). Please note that \n",
    "# we will evaluate your implementations only using CPU mode so even if\n",
    "# you use a GPU, make sure your code runs in the CPU mode with the\n",
    "# environment we provided. \n",
    "use_GPU = True\n",
    "if use_GPU:\n",
    "    from utils_gpu import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a network in PyTorch, we need 4 components:\n",
    "1. **Dataset** - an object which can load the data and labels given an index.\n",
    "2. **Model** - an object that contains the network architecture definition.\n",
    "3. **Loss function** - a function that measures how far the network output is from the ground truth label.\n",
    "4. **Optimizer** - an object that optimizes the network parameters to reduce the loss value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has two main parts. In Part 1, you will train a deep network from scratch. In Part 2, you will \"fine-tune\" a trained network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0. Warm up! Training a Deep Network from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fix random seeds so that results will be reproducible\n",
    "# set_seed(0, use_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not need to code anything for this part. You will simply run the code we provided, but we want you to report the result you got. This section will also familiarize you with the steps of training a deep network from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters.\n",
    "input_size = (64, 64)\n",
    "RGB = False\n",
    "base_lr = 0.001  # may try a smaller lr if not using batch norm\n",
    "weight_decay = 5e-4\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create our datasets, by calling the create_datasets function from student_code. This function returns a separate dataset loader for each split of the dataset (training and testing/validation). Each dataloader is used to load the datasets after appling some pre-processing transforms. In Part 1, you will be asked to add a few more pre-processing transforms to the dataloaders by modifying this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create the training and testing datasets.\n",
    "# train_dataset, test_dataset = sc.create_datasets(data_path=data_path, input_size=input_size, rgb=RGB)\n",
    "# assert test_dataset.classes == train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create our network model using the SimpleNet class from student_code. The implementation provided in the SimpleNet class gives you a basic network. In Part 1, you will be asked to add a few more layers to this network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Create the network model.\n",
    "# model = sc.SimpleNet(num_classes=num_classes, rgb=False, verbose=False)\n",
    "# if use_GPU:\n",
    "#     model = model.cuda()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the loss function and the optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the loss function.\n",
    "# # see http://pytorch.org/docs/0.3.0/nn.html#loss-functions for a list of available loss functions\n",
    "# loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the optimizer and a learning rate scheduler\n",
    "# optimizer = optim.SGD(params=model.parameters(), lr=base_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "# # Currently a simple step scheduler.\n",
    "# # See http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate for various LR schedulers\n",
    "# # and how to use them\n",
    "# lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we are ready to train our network! We will start a local server to see the training progress of our network. Open a new terminal and activate the environment for this project. Then run the following command: **python -m visdom.server**. This will start a local server. The terminal output should give out a link like: \"http://localhost:8097\". Open this link in your browser. After you run the following block, visit this link again, and you will be able to see graphs showing the progress of your training! If you do not see any graphs, select Part 1 on the top left bar where is says Environment (only select Part 1, do not check main or Part 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # train the network!\n",
    "# params = {'n_epochs': 100, 'batch_size': 50, 'experiment': 'part1'}\n",
    "# trainer = Trainer(train_dataset, test_dataset, model, loss_function, optimizer, lr_scheduler, params)\n",
    "# best_prec1 = trainer.train_val()\n",
    "# print('Best top-1 Accuracy = {:4.3f}'.format(best_prec1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect this code to take around 5 minutes on CPU or 3 minutes on GPU. Now you are ready to actually modify the functions we used to train our model. Before you move on, make sure to record the accuracy of your network from Part 0, and report it in your write up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Modifying the Dataloaders and the Simple Network create_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds so that results will be reproducible\n",
    "set_seed(0, use_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will modify the create_datasets function from student_code. You will add random left-right mirroring and normalization to the transformations applied to the training dataset. You will also add normalization to the transformations applied to the testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 30\n",
      "Batch 20 / 30\n",
      "Done, mean = \n",
      "[0.45579668]\n",
      "std = \n",
      "[0.23624939]\n",
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 60\n",
      "Batch 20 / 60\n",
      "Batch 40 / 60\n",
      "Done, mean = \n",
      "[0.45517009]\n",
      "std = \n",
      "[0.2350788]\n"
     ]
    }
   ],
   "source": [
    "# Create the training and testing datasets.\n",
    "train_dataset, test_dataset = sc.create_datasets(data_path=data_path, input_size=input_size, rgb=RGB)\n",
    "assert test_dataset.classes == train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will modify SimpleNet by adding droppout, batch normalization, and additional convolution/maxpool/relu layers. You should achieve an accuracy of at least **50%**. Make sure your network passes this threshold--it is required for full credit on this section!\n",
    "\n",
    "You can also use the following two blocks to determine the stucture of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SimpleNet(\n  (features): Sequential(\n    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (6): ReLU(inplace=True)\n    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (10): ReLU(inplace=True)\n    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (12): Dropout(p=0.3, inplace=False)\n    (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (15): ReLU(inplace=True)\n    (16): Dropout(p=0.3, inplace=False)\n  )\n  (classifier): Conv2d(256, 15, kernel_size=(4, 4), stride=(1, 1))\n)\n"
     ]
    }
   ],
   "source": [
    "# create the network model\n",
    "model = sc.SimpleNet(num_classes=num_classes, rgb=False, verbose=False)\n",
    "if use_GPU:\n",
    "    model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Network output size is  torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "# Use this block to determine the kernel size of the conv2d layer in the classifier\n",
    "# first, set the kernel size of that conv2d layer to 1, and run this block\n",
    "# then, use that size of input to the classifier printed by this block to\n",
    "# go back and update the kernel size of the conv2d layer in the classifier\n",
    "# Finally, run this block again and verify that the network output size is a scalar\n",
    "# Don't forget to re-run the block above every time you update the SimpleNet class!\n",
    "from torch.autograd import Variable\n",
    "data, _ = train_dataset[0]\n",
    "s = data.size()\n",
    "data = Variable(data.view(1, *s))\n",
    "if use_GPU:\n",
    "    data = data.cuda()\n",
    "out = model(data)\n",
    "print('Network output size is ', out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the loss function and the optimizer. You do not have to modify the custom_part1_trainer in student_code if you use the same loss_function, optimizer, scheduler and parameters (n_epoch, batch_size etc.) as provided in this notebook to hit the required threshold of 50% accuracy. If you changed any of these values, it is important that you modify this function in student_code since we will not be using the notebook you submit to evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the trainer. You can modify custom_part1_trainer in\n",
    "# student_copy.py if you want to try different learning settings.\n",
    "custom_part1_trainer = sc.custom_part1_trainer(model)\n",
    "\n",
    "if custom_part1_trainer is None:\n",
    "    # Create the loss function.\n",
    "    # see http://pytorch.org/docs/0.3.0/nn.html#loss-functions for a list of available loss functions\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create the optimizer and a learning rate scheduler.\n",
    "    optimizer = optim.SGD(params=model.parameters(), lr=base_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "    # Currently a simple step scheduler, but you can get creative.\n",
    "    # See http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate for various LR schedulers\n",
    "    # and how to use them\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=60, gamma=0.1)\n",
    "\n",
    "    params = {'n_epochs': 100, 'batch_size': 50, 'experiment': 'part1'}\n",
    "    \n",
    "else:\n",
    "    if 'loss_function' in custom_part1_trainer:\n",
    "        loss_function = custom_part1_trainer['loss_function']\n",
    "    if 'optimizer' in custom_part1_trainer:\n",
    "        optimizer = custom_part1_trainer['optimizer']\n",
    "    if 'lr_scheduler' in custom_part1_trainer:\n",
    "        lr_scheduler = custom_part1_trainer['lr_scheduler']\n",
    "    if 'params' in custom_part1_trainer:\n",
    "        params = custom_part1_trainer['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train our network! As before, we will start a local server to see the training progress of our network (if you server is already running, you should not start another one). Open a new terminal and activate the environment for this project. Then run the following command: **python -m visdom.server**. This will start a local server. The terminal output should give out a link like: \"http://localhost:8097\". Open this link in your browser. After you run the following block, visit this link again, and you will be able to see graphs showing the progress of your training! If you do not see any graphs, select Part 1 on the top left bar where is says Environment (only select Part 1, do not check main or Part 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "train part1: loss 0.496274\n",
      "val part1: batch 0/59, loss 1.885, top-1 accuracy 28.000, top-5 accuracy 86.000\n",
      "val part1: loss 1.180307\n",
      "Checkpoint saved\n",
      "part1 Epoch 23 / 100\n",
      "train part1: batch 0/29, loss 0.467, top-1 accuracy 92.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.454896\n",
      "val part1: batch 0/59, loss 1.973, top-1 accuracy 34.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.337974\n",
      "Checkpoint saved\n",
      "part1 Epoch 24 / 100\n",
      "train part1: batch 0/29, loss 0.540, top-1 accuracy 88.000, top-5 accuracy 98.000\n",
      "train part1: loss 0.464526\n",
      "val part1: batch 0/59, loss 1.574, top-1 accuracy 44.000, top-5 accuracy 94.000\n",
      "val part1: loss 1.162171\n",
      "Checkpoint saved\n",
      "part1 Epoch 25 / 100\n",
      "train part1: batch 0/29, loss 0.288, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.430548\n",
      "val part1: batch 0/59, loss 2.155, top-1 accuracy 34.000, top-5 accuracy 80.000\n",
      "val part1: loss 1.378292\n",
      "Checkpoint saved\n",
      "part1 Epoch 26 / 100\n",
      "train part1: batch 0/29, loss 0.294, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.412057\n",
      "val part1: batch 0/59, loss 1.718, top-1 accuracy 36.000, top-5 accuracy 90.000\n",
      "val part1: loss 1.232948\n",
      "Checkpoint saved\n",
      "part1 Epoch 27 / 100\n",
      "train part1: batch 0/29, loss 0.333, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.380247\n",
      "val part1: batch 0/59, loss 2.368, top-1 accuracy 18.000, top-5 accuracy 76.000\n",
      "val part1: loss 1.227330\n",
      "Checkpoint saved\n",
      "part1 Epoch 28 / 100\n",
      "train part1: batch 0/29, loss 0.300, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.374242\n",
      "val part1: batch 0/59, loss 2.247, top-1 accuracy 24.000, top-5 accuracy 86.000\n",
      "val part1: loss 1.231555\n",
      "Checkpoint saved\n",
      "part1 Epoch 29 / 100\n",
      "train part1: batch 0/29, loss 0.299, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.317298\n",
      "val part1: batch 0/59, loss 2.035, top-1 accuracy 34.000, top-5 accuracy 80.000\n",
      "val part1: loss 1.082427\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 30 / 100\n",
      "train part1: batch 0/29, loss 0.293, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.274764\n",
      "val part1: batch 0/59, loss 2.061, top-1 accuracy 28.000, top-5 accuracy 80.000\n",
      "val part1: loss 1.067336\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 31 / 100\n",
      "train part1: batch 0/29, loss 0.296, top-1 accuracy 92.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.288064\n",
      "val part1: batch 0/59, loss 1.873, top-1 accuracy 36.000, top-5 accuracy 80.000\n",
      "val part1: loss 1.062480\n",
      "Checkpoint saved\n",
      "part1 Epoch 32 / 100\n",
      "train part1: batch 0/29, loss 0.306, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.272818\n",
      "val part1: batch 0/59, loss 2.003, top-1 accuracy 32.000, top-5 accuracy 80.000\n",
      "val part1: loss 1.067890\n",
      "Checkpoint saved\n",
      "part1 Epoch 33 / 100\n",
      "train part1: batch 0/29, loss 0.293, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.260350\n",
      "val part1: batch 0/59, loss 1.935, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.065257\n",
      "Checkpoint saved\n",
      "part1 Epoch 34 / 100\n",
      "train part1: batch 0/29, loss 0.245, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.271907\n",
      "val part1: batch 0/59, loss 1.913, top-1 accuracy 34.000, top-5 accuracy 80.000\n",
      "val part1: loss 1.055918\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 35 / 100\n",
      "train part1: batch 0/29, loss 0.321, top-1 accuracy 88.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.260127\n",
      "val part1: batch 0/59, loss 1.895, top-1 accuracy 34.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.067395\n",
      "Checkpoint saved\n",
      "part1 Epoch 36 / 100\n",
      "train part1: batch 0/29, loss 0.385, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.264089\n",
      "val part1: batch 0/59, loss 1.963, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.074034\n",
      "Checkpoint saved\n",
      "part1 Epoch 37 / 100\n",
      "train part1: batch 0/29, loss 0.286, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.262140\n",
      "val part1: batch 0/59, loss 1.922, top-1 accuracy 38.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.057696\n",
      "Checkpoint saved\n",
      "part1 Epoch 38 / 100\n",
      "train part1: batch 0/29, loss 0.282, top-1 accuracy 96.000, top-5 accuracy 98.000\n",
      "train part1: loss 0.256258\n",
      "val part1: batch 0/59, loss 1.926, top-1 accuracy 34.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.053090\n",
      "Checkpoint saved\n",
      "part1 Epoch 39 / 100\n",
      "train part1: batch 0/29, loss 0.268, top-1 accuracy 92.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.252701\n",
      "val part1: batch 0/59, loss 1.984, top-1 accuracy 32.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.061071\n",
      "Checkpoint saved\n",
      "part1 Epoch 40 / 100\n",
      "train part1: batch 0/29, loss 0.236, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.254884\n",
      "val part1: batch 0/59, loss 1.924, top-1 accuracy 32.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.057098\n",
      "Checkpoint saved\n",
      "part1 Epoch 41 / 100\n",
      "train part1: batch 0/29, loss 0.278, top-1 accuracy 90.000, top-5 accuracy 98.000\n",
      "train part1: loss 0.260978\n",
      "val part1: batch 0/59, loss 1.947, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.051529\n",
      "Checkpoint saved\n",
      "part1 Epoch 42 / 100\n",
      "train part1: batch 0/29, loss 0.175, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.236923\n",
      "val part1: batch 0/59, loss 1.915, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.052523\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 43 / 100\n",
      "train part1: batch 0/29, loss 0.207, top-1 accuracy 100.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.245456\n",
      "val part1: batch 0/59, loss 1.946, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.052777\n",
      "Checkpoint saved\n",
      "part1 Epoch 44 / 100\n",
      "train part1: batch 0/29, loss 0.248, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.238952\n",
      "val part1: batch 0/59, loss 1.926, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.053719\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 45 / 100\n",
      "train part1: batch 0/29, loss 0.281, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.238808\n",
      "val part1: batch 0/59, loss 1.951, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.047803\n",
      "Checkpoint saved\n",
      "part1 Epoch 46 / 100\n",
      "train part1: batch 0/29, loss 0.190, top-1 accuracy 100.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.239105\n",
      "val part1: batch 0/59, loss 1.908, top-1 accuracy 40.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.052547\n",
      "Checkpoint saved\n",
      "part1 Epoch 47 / 100\n",
      "train part1: batch 0/29, loss 0.229, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.230683\n",
      "val part1: batch 0/59, loss 1.940, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.053038\n",
      "Checkpoint saved\n",
      "part1 Epoch 48 / 100\n",
      "train part1: batch 0/29, loss 0.241, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.239133\n",
      "val part1: batch 0/59, loss 2.042, top-1 accuracy 34.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.051873\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 49 / 100\n",
      "train part1: batch 0/29, loss 0.264, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.233881\n",
      "val part1: batch 0/59, loss 1.967, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.047500\n",
      "Checkpoint saved\n",
      "part1 Epoch 50 / 100\n",
      "train part1: batch 0/29, loss 0.273, top-1 accuracy 96.000, top-5 accuracy 98.000\n",
      "train part1: loss 0.228838\n",
      "val part1: batch 0/59, loss 1.968, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.042194\n",
      "Checkpoint saved\n",
      "part1 Epoch 51 / 100\n",
      "train part1: batch 0/29, loss 0.196, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.223282\n",
      "val part1: batch 0/59, loss 2.051, top-1 accuracy 32.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.046222\n",
      "Checkpoint saved\n",
      "part1 Epoch 52 / 100\n",
      "train part1: batch 0/29, loss 0.270, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.242003\n",
      "val part1: batch 0/59, loss 2.015, top-1 accuracy 34.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.050173\n",
      "Checkpoint saved\n",
      "part1 Epoch 53 / 100\n",
      "train part1: batch 0/29, loss 0.342, top-1 accuracy 88.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.226577\n",
      "val part1: batch 0/59, loss 1.892, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.048836\n",
      "Checkpoint saved\n",
      "part1 Epoch 54 / 100\n",
      "train part1: batch 0/29, loss 0.189, top-1 accuracy 98.000, top-5 accuracy 98.000\n",
      "train part1: loss 0.223298\n",
      "val part1: batch 0/59, loss 1.974, top-1 accuracy 34.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.044217\n",
      "Checkpoint saved\n",
      "part1 Epoch 55 / 100\n",
      "train part1: batch 0/29, loss 0.186, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.222348\n",
      "val part1: batch 0/59, loss 2.084, top-1 accuracy 32.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.052064\n",
      "Checkpoint saved\n",
      "part1 Epoch 56 / 100\n",
      "train part1: batch 0/29, loss 0.184, top-1 accuracy 100.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.229279\n",
      "val part1: batch 0/59, loss 2.050, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.044873\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 57 / 100\n",
      "train part1: batch 0/29, loss 0.135, top-1 accuracy 100.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.211946\n",
      "val part1: batch 0/59, loss 1.949, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.061995\n",
      "Checkpoint saved\n",
      "part1 Epoch 58 / 100\n",
      "train part1: batch 0/29, loss 0.155, top-1 accuracy 100.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.215554\n",
      "val part1: batch 0/59, loss 2.015, top-1 accuracy 34.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.049644\n",
      "Checkpoint saved\n",
      "part1 Epoch 59 / 100\n",
      "train part1: batch 0/29, loss 0.297, top-1 accuracy 92.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.216697\n",
      "val part1: batch 0/59, loss 1.991, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.042844\n",
      "Checkpoint saved\n",
      "part1 Epoch 60 / 100\n",
      "train part1: batch 0/29, loss 0.243, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.226359\n",
      "val part1: batch 0/59, loss 1.972, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.043319\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 61 / 100\n",
      "train part1: batch 0/29, loss 0.243, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.206917\n",
      "val part1: batch 0/59, loss 2.010, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.041592\n",
      "Checkpoint saved\n",
      "part1 Epoch 62 / 100\n",
      "train part1: batch 0/29, loss 0.295, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.214405\n",
      "val part1: batch 0/59, loss 1.977, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.046780\n",
      "Checkpoint saved\n",
      "part1 Epoch 63 / 100\n",
      "train part1: batch 0/29, loss 0.253, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.205963\n",
      "val part1: batch 0/59, loss 1.975, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.044429\n",
      "Checkpoint saved\n",
      "part1 Epoch 64 / 100\n",
      "train part1: batch 0/29, loss 0.203, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.215953\n",
      "val part1: batch 0/59, loss 2.019, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.043462\n",
      "Checkpoint saved\n",
      "part1 Epoch 65 / 100\n",
      "train part1: batch 0/29, loss 0.336, top-1 accuracy 92.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.204830\n",
      "val part1: batch 0/59, loss 1.972, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.040548\n",
      "Checkpoint saved\n",
      "part1 Epoch 66 / 100\n",
      "train part1: batch 0/29, loss 0.236, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.211310\n",
      "val part1: batch 0/59, loss 1.995, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.042029\n",
      "Checkpoint saved\n",
      "part1 Epoch 67 / 100\n",
      "train part1: batch 0/29, loss 0.292, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.209287\n",
      "val part1: batch 0/59, loss 1.981, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.039873\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 68 / 100\n",
      "train part1: batch 0/29, loss 0.312, top-1 accuracy 90.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.206943\n",
      "val part1: batch 0/59, loss 1.953, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.041129\n",
      "Checkpoint saved\n",
      "part1 Epoch 69 / 100\n",
      "train part1: batch 0/29, loss 0.188, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.209862\n",
      "val part1: batch 0/59, loss 1.949, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.039431\n",
      "Checkpoint saved\n",
      "part1 Epoch 70 / 100\n",
      "train part1: batch 0/29, loss 0.223, top-1 accuracy 96.000, top-5 accuracy 98.000\n",
      "train part1: loss 0.212378\n",
      "val part1: batch 0/59, loss 1.979, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.038338\n",
      "Checkpoint saved\n",
      "part1 Epoch 71 / 100\n",
      "train part1: batch 0/29, loss 0.194, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.211790\n",
      "val part1: batch 0/59, loss 2.002, top-1 accuracy 34.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.040260\n",
      "Checkpoint saved\n",
      "part1 Epoch 72 / 100\n",
      "train part1: batch 0/29, loss 0.247, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.205089\n",
      "val part1: batch 0/59, loss 2.045, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.040105\n",
      "Checkpoint saved\n",
      "part1 Epoch 73 / 100\n",
      "train part1: batch 0/29, loss 0.285, top-1 accuracy 92.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.209957\n",
      "val part1: batch 0/59, loss 2.005, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.036598\n",
      "Checkpoint saved\n",
      "part1 Epoch 74 / 100\n",
      "train part1: batch 0/29, loss 0.228, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.201638\n",
      "val part1: batch 0/59, loss 1.962, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.041338\n",
      "Checkpoint saved\n",
      "part1 Epoch 75 / 100\n",
      "train part1: batch 0/29, loss 0.266, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.220756\n",
      "val part1: batch 0/59, loss 1.957, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.037402\n",
      "Checkpoint saved\n",
      "part1 Epoch 76 / 100\n",
      "train part1: batch 0/29, loss 0.184, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.206312\n",
      "val part1: batch 0/59, loss 1.963, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.038829\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 77 / 100\n",
      "train part1: batch 0/29, loss 0.153, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.209140\n",
      "val part1: batch 0/59, loss 1.989, top-1 accuracy 38.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.038335\n",
      "Checkpoint saved\n",
      "part1 Epoch 78 / 100\n",
      "train part1: batch 0/29, loss 0.156, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.218300\n",
      "val part1: batch 0/59, loss 1.979, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.039404\n",
      "Checkpoint saved\n",
      "part1 Epoch 79 / 100\n",
      "train part1: batch 0/29, loss 0.175, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.217653\n",
      "val part1: batch 0/59, loss 1.907, top-1 accuracy 40.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.043941\n",
      "Checkpoint saved\n",
      "part1 Epoch 80 / 100\n",
      "train part1: batch 0/29, loss 0.192, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.201693\n",
      "val part1: batch 0/59, loss 1.988, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.038838\n",
      "Checkpoint saved\n",
      "part1 Epoch 81 / 100\n",
      "train part1: batch 0/29, loss 0.235, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.208447\n",
      "val part1: batch 0/59, loss 1.971, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.044668\n",
      "Checkpoint saved\n",
      "part1 Epoch 82 / 100\n",
      "train part1: batch 0/29, loss 0.185, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.214455\n",
      "val part1: batch 0/59, loss 2.025, top-1 accuracy 34.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.039803\n",
      "Checkpoint saved\n",
      "part1 Epoch 83 / 100\n",
      "train part1: batch 0/29, loss 0.299, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.214388\n",
      "val part1: batch 0/59, loss 2.013, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.040187\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 84 / 100\n",
      "train part1: batch 0/29, loss 0.208, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.206136\n",
      "val part1: batch 0/59, loss 1.983, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.038821\n",
      "Checkpoint saved\n",
      "part1 Epoch 85 / 100\n",
      "train part1: batch 0/29, loss 0.286, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.205797\n",
      "val part1: batch 0/59, loss 1.983, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.039505\n",
      "Checkpoint saved\n",
      "part1 Epoch 86 / 100\n",
      "train part1: batch 0/29, loss 0.205, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.210980\n",
      "val part1: batch 0/59, loss 1.973, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.039486\n",
      "Checkpoint saved\n",
      "part1 Epoch 87 / 100\n",
      "train part1: batch 0/29, loss 0.243, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.211399\n",
      "val part1: batch 0/59, loss 2.040, top-1 accuracy 34.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.035821\n",
      "Checkpoint saved\n",
      "part1 Epoch 88 / 100\n",
      "train part1: batch 0/29, loss 0.153, top-1 accuracy 100.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.206398\n",
      "val part1: batch 0/59, loss 2.023, top-1 accuracy 34.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.035616\n",
      "Checkpoint saved\n",
      "part1 Epoch 89 / 100\n",
      "train part1: batch 0/29, loss 0.205, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.215057\n",
      "val part1: batch 0/59, loss 2.003, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.041943\n",
      "Checkpoint saved\n",
      "part1 Epoch 90 / 100\n",
      "train part1: batch 0/29, loss 0.321, top-1 accuracy 90.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.210812\n",
      "val part1: batch 0/59, loss 1.990, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.037600\n",
      "Checkpoint saved\n",
      "part1 Epoch 91 / 100\n",
      "train part1: batch 0/29, loss 0.238, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.205665\n",
      "val part1: batch 0/59, loss 2.025, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.039602\n",
      "Checkpoint saved\n",
      "part1 Epoch 92 / 100\n",
      "train part1: batch 0/29, loss 0.198, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.207782\n",
      "val part1: batch 0/59, loss 2.004, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.036325\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 93 / 100\n",
      "train part1: batch 0/29, loss 0.232, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.202940\n",
      "val part1: batch 0/59, loss 1.996, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.037853\n",
      "Checkpoint saved\n",
      "part1 Epoch 94 / 100\n",
      "train part1: batch 0/29, loss 0.261, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.214926\n",
      "val part1: batch 0/59, loss 2.009, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.039873\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part1 Epoch 95 / 100\n",
      "train part1: batch 0/29, loss 0.252, top-1 accuracy 94.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.201180\n",
      "val part1: batch 0/59, loss 2.023, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.038965\n",
      "Checkpoint saved\n",
      "part1 Epoch 96 / 100\n",
      "train part1: batch 0/29, loss 0.176, top-1 accuracy 100.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.197453\n",
      "val part1: batch 0/59, loss 1.985, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.038547\n",
      "Checkpoint saved\n",
      "part1 Epoch 97 / 100\n",
      "train part1: batch 0/29, loss 0.200, top-1 accuracy 96.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.206349\n",
      "val part1: batch 0/59, loss 1.995, top-1 accuracy 36.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.042809\n",
      "Checkpoint saved\n",
      "part1 Epoch 98 / 100\n",
      "train part1: batch 0/29, loss 0.159, top-1 accuracy 100.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.208483\n",
      "val part1: batch 0/59, loss 2.038, top-1 accuracy 38.000, top-5 accuracy 84.000\n",
      "val part1: loss 1.037754\n",
      "Checkpoint saved\n",
      "part1 Epoch 99 / 100\n",
      "train part1: batch 0/29, loss 0.217, top-1 accuracy 98.000, top-5 accuracy 100.000\n",
      "train part1: loss 0.202377\n",
      "val part1: batch 0/59, loss 2.024, top-1 accuracy 36.000, top-5 accuracy 82.000\n",
      "val part1: loss 1.035295\n",
      "Checkpoint saved\n",
      "Best top-1 Accuracy = 67.672\n"
     ]
    }
   ],
   "source": [
    "# Train the network!\n",
    "trainer = Trainer(train_dataset, test_dataset, model, loss_function, optimizer, lr_scheduler, params)\n",
    "best_prec1 = trainer.train_val()\n",
    "print('Best top-1 Accuracy = {:4.3f}'.format(best_prec1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you get at least 50% accuracy in this section! If you tried different settings than the ones provided to get 50%, you should modify custom_part1_trainer in student code to return a dictionary with your changed settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Fine-Tuning a Pre-Trained Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix random seeds so that results will be reproducible\n",
    "set_seed(0, use_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a network from scratch takes a lof of time. Instead of training from scratch, we can take a pre-trained model and fine tune it for our purposes. This is the goal of Part 2--you will train a pre-trained network, and achieve at least 80% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "input_size = (224, 224)\n",
    "RGB = True\n",
    "base_lr = 1e-3\n",
    "weight_decay = 5e-4\n",
    "momentum = 0.9\n",
    "backprop_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 30\n",
      "Batch 20 / 30\n",
      "Done, mean = \n",
      "[0.45586014 0.45586014 0.45586014]\n",
      "std = \n",
      "[0.24808612 0.24808612 0.24808612]\n",
      "Computing pixel mean and stdev...\n",
      "Batch 0 / 60\n",
      "Batch 20 / 60\n",
      "Batch 40 / 60\n",
      "Done, mean = \n",
      "[0.45524448 0.45524448 0.45524448]\n",
      "std = \n",
      "[0.24719196 0.24719196 0.24719196]\n"
     ]
    }
   ],
   "source": [
    "# Create the training and testing datasets.\n",
    "train_dataset, test_dataset = sc.create_datasets(data_path=data_path, input_size=input_size, rgb=RGB)\n",
    "assert test_dataset.classes == train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following block loads a pretrained AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=1000, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "# Create the network model.\n",
    "model = alexnet(pretrained=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you modify create_part2_model from student code in order to fine-tune AlexNet. As you can see in the docs (https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py) and in the model printout above, AlexNet has 2 parts: 'features', which constists of conv layers that extract feature maps from the image, and 'classifier' which consists of FC layers that classify the features. We want to replace the last Linear layer in model.classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "AlexNet(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (9): ReLU(inplace=True)\n    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU(inplace=True)\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU(inplace=True)\n    (6): Linear(in_features=4096, out_features=15, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "model = sc.create_part2_model(model, num_classes)\n",
    "if use_GPU:\n",
    "    model = model.cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will create the loss function and the optimizer. Just as with part 1, if you modify any of the setttings to hit the required accuracy, you must modify custom_part2_trainer function to return a dictionary containing your changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the trainer. You can modify custom_part2_trainer in\n",
    "# student_copy.py if you want to try different learning settings.\n",
    "custom_part2_trainer = sc.custom_part2_trainer(model)\n",
    "\n",
    "if custom_part2_trainer is None:\n",
    "    # Create the loss function\n",
    "    # see http://pytorch.org/docs/0.3.0/nn.html#loss-functions for a list of available loss functions\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Since we do not want to optimize the whole network, we must extract a list of parameters of interest that will be\n",
    "    # optimized by the optimizer.\n",
    "    params_to_optimize = []\n",
    "\n",
    "    # List of modules in the network\n",
    "    mods = list(model.features.children()) + list(model.classifier.children())\n",
    "\n",
    "    # Extract parameters from the last `backprop_depth` modules in the network and collect them in\n",
    "    # the params_to_optimize list.\n",
    "    for m in mods[::-1][:backprop_depth]:\n",
    "        params_to_optimize.extend(list(m.parameters()))\n",
    "\n",
    "    # Construct the optimizer    \n",
    "    optimizer = optim.SGD(params=params_to_optimize, lr=base_lr, weight_decay=weight_decay, momentum=momentum)\n",
    "\n",
    "    # Create a scheduler, currently a simple step scheduler, but you can get creative.\n",
    "    # See http://pytorch.org/docs/0.3.0/optim.html#how-to-adjust-learning-rate for various LR schedulers\n",
    "    # and how to use them\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    \n",
    "    params = {'n_epochs': 4, 'batch_size': 10, 'experiment': 'part2'} \n",
    "    \n",
    "else:\n",
    "    if 'loss_function' in custom_part2_trainer:\n",
    "        loss_function = custom_part2_trainer['loss_function']\n",
    "    if 'optimizer' in custom_part2_trainer:\n",
    "        optimizer = custom_part2_trainer['optimizer']\n",
    "    if 'lr_scheduler' in custom_part2_trainer:\n",
    "        lr_scheduler = custom_part2_trainer['lr_scheduler']\n",
    "    if 'params' in custom_part2_trainer:\n",
    "        params = custom_part2_trainer['params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to fine tune our network! Just like before, we will start a local server to see the training progress of our network. Open a new terminal and activate the environment for this project. Then run the following command: **python -m visdom.server**. This will start a local server. The terminal output should give out a link like: \"http://localhost:8097\". Open this link in your browser. After you run the following block, visit this link again, and you will be able to see graphs showing the progress of your training! If you do not see any graphs, select Part 2 on the top left bar where is says Environment (only select Part 2, do not check main or Part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting up a new session...\n",
      "---------------------------------------\n",
      "Experiment: part2\n",
      "n_epochs: 4\n",
      "batch_size: 10\n",
      "do_val: True\n",
      "shuffle: True\n",
      "num_workers: 4\n",
      "val_freq: 1\n",
      "print_freq: 100\n",
      "experiment: part2\n",
      "checkpoint_file: None\n",
      "resume_optim: True\n",
      "---------------------------------------\n",
      "part2 Epoch 0 / 4\n",
      "train part2: batch 0/149, loss 2.807, top-1 accuracy 20.000, top-5 accuracy 40.000\n",
      "train part2: batch 100/149, loss 1.434, top-1 accuracy 70.000, top-5 accuracy 90.000\n",
      "train part2: loss 0.941981\n",
      "val part2: batch 0/298, loss 0.602, top-1 accuracy 80.000, top-5 accuracy 100.000\n",
      "val part2: batch 100/298, loss 0.193, top-1 accuracy 90.000, top-5 accuracy 100.000\n",
      "val part2: batch 200/298, loss 0.352, top-1 accuracy 70.000, top-5 accuracy 100.000\n",
      "val part2: loss 0.505572\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part2 Epoch 1 / 4\n",
      "train part2: batch 0/149, loss 0.111, top-1 accuracy 100.000, top-5 accuracy 100.000\n",
      "train part2: batch 100/149, loss 0.489, top-1 accuracy 80.000, top-5 accuracy 100.000\n",
      "train part2: loss 0.369040\n",
      "val part2: batch 0/298, loss 0.535, top-1 accuracy 70.000, top-5 accuracy 100.000\n",
      "val part2: batch 100/298, loss 0.287, top-1 accuracy 90.000, top-5 accuracy 100.000\n",
      "val part2: batch 200/298, loss 1.083, top-1 accuracy 60.000, top-5 accuracy 100.000\n",
      "val part2: loss 0.444958\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "part2 Epoch 2 / 4\n",
      "train part2: batch 0/149, loss 0.133, top-1 accuracy 100.000, top-5 accuracy 100.000\n",
      "train part2: batch 100/149, loss 0.468, top-1 accuracy 80.000, top-5 accuracy 100.000\n",
      "train part2: loss 0.258265\n",
      "val part2: batch 0/298, loss 1.158, top-1 accuracy 60.000, top-5 accuracy 100.000\n",
      "val part2: batch 100/298, loss 0.209, top-1 accuracy 90.000, top-5 accuracy 100.000\n",
      "val part2: batch 200/298, loss 0.497, top-1 accuracy 80.000, top-5 accuracy 100.000\n",
      "val part2: loss 0.449708\n",
      "Checkpoint saved\n",
      "part2 Epoch 3 / 4\n",
      "train part2: batch 0/149, loss 0.229, top-1 accuracy 90.000, top-5 accuracy 100.000\n",
      "train part2: batch 100/149, loss 0.112, top-1 accuracy 100.000, top-5 accuracy 100.000\n",
      "train part2: loss 0.186321\n",
      "val part2: batch 0/298, loss 1.191, top-1 accuracy 50.000, top-5 accuracy 100.000\n",
      "val part2: batch 100/298, loss 0.655, top-1 accuracy 90.000, top-5 accuracy 100.000\n",
      "val part2: batch 200/298, loss 0.953, top-1 accuracy 60.000, top-5 accuracy 100.000\n",
      "val part2: loss 0.440472\n",
      "Checkpoint saved\n",
      "BEST TOP1 ACCURACY SO FAR\n",
      "Best top-1 Accuracy = 85.327\n"
     ]
    }
   ],
   "source": [
    "# Train the network!\n",
    "trainer = Trainer(train_dataset, test_dataset, model, loss_function, optimizer, lr_scheduler, params)\n",
    "best_prec1 = trainer.train_val()\n",
    "print('Best top-1 Accuracy = {:4.3f}'.format(best_prec1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expect this code to take around 10 minutes on CPU or 30 seconds on GPU. You should hit 80% accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('tensorflow': conda)",
   "metadata": {
    "interpreter": {
     "hash": "91e96b0a3d300565f1465e18a089ebc68626de02f710436976b5c5edb915c12a"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}